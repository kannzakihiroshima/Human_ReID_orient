# 人物の体の向きを考慮した人物再同定手法
これは，第70回土木計画学研究発表会・秋大会で発表した論文のコードである

### 使用データ
- 広島大学内に設置された６台のカメラ（各カメラの画角は被らない）
- 事前に集めた被験者６人にGPSをつけ，一般歩行者に紛れて歩行させる
- アノテーションは，ビデオに映った被験者のみに固有ラベルを付し，その他歩行者には【未知】ラベルを付与する．よって計７つのラベルを付与する．

### 手順
##### 体の向き判定モデル準備
1. Market-1501の人物画像に対して体の向きラベルのアノテーション

##### 全体フロー
1. 撮影動画をStrongSORT-YOLOに入力し人物画像を切り出す．また追跡結果より同一人物と思われる画像群であるトラックレットを得る（track.ipynb）
2. すべての画像をmediapipe-poseに入力し，骨格特徴量を得る．また，骨格特徴量を基に基準を満たしていない低品質なサンプルを訓練データから削除する(mediapipe.ipynb)（理由は山崎の論文参照）
3. すべての画像にデータオーギュメンテーションを適用する（  data_augumentation.ipynb）
4. すべての画像をOSNet(特徴量抽出部分のみ使用)に入力し，外観特徴量を得る（osnet.ipynb）
5. 骨格特徴量を基に人物の体の向きを予測するモデルAを学習する．（）
6. OSNetから得た外観特徴量とモデルAから得た体の向き特徴量を基に人物・体の向きをベクトル空間上で分離するようにDNNを距離学習する（モデルB）.（損失関数 ArcFace Loss Norm Loss）()
7. 距離学習モデルBを用いて，外観特徴量と体の向き特徴量の合成特徴量を変換する，（）
8. 7で出力された特徴量を基に最適輸送を使用して，トラックレット単位でギャラリーデータとクエリデータのベクトル空間上の距離を計算する．結果を用いて各トラックレットの人物ラベルを決定する()


   
![スクリーンショット 2024-11-05 182027](https://github.com/user-attachments/assets/4e8a98e6-13f5-4cbf-ac76-73c5444ea8aa)

![スクリーンショット 2024-11-05 182110](https://github.com/user-attachments/assets/ca836dd4-37bd-4105-bca8-3677ac8cfb5e)


![スクリーンショット 2024-11-05 180056](https://github.com/user-attachments/assets/0456ea19-9ab2-44b6-9e99-c3be6ac53283)

![スクリーンショット 2024-11-05 171226](https://github.com/user-attachments/assets/454ab62e-d377-4b28-9675-358d89c1ad67)

![スクリーンショット 2024-11-05 171700](https://github.com/user-attachments/assets/d821ad8a-8354-46a5-8196-f8cfa7d0b636)
