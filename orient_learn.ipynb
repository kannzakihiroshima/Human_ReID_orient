{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd86329-11c9-4ff5-8aec-5847e1e9e314",
   "metadata": {},
   "source": [
    "# 人物の体の向きを予測するモデルAを学習する\n",
    "\n",
    "#### 手順\n",
    "\n",
    "1. 元データセット（Market-1501）内の画像全部にmedia pipeを適用し，姿勢が検出された画像ファイルリストを作成する．Find_possible_jp.py\n",
    "2. １のリストを基に姿勢が検出された画像フォルダを作成する　　Find_possible_jp.py\n",
    "3. ２のフォルダをlabel-studioにインポートする\n",
    "4. label-studioでアノテーションする（別のワードファイルで説明）\n",
    "5. アノテーション結果csvファイルからアノテーション済みの画像ファイルを取得　　Image_list.py\n",
    "6. 5のパスリストを基にアノテーション済みの画像フォルダを作成する　　Image_list.py\n",
    "7. 7で作成したフォルダ内の画像にmediapipeを適用する（１．でやっているが，なぜか2回目適用しています．本当は1の結果から抜き出せばいいのに...）　Media_pipe_anotation.py\n",
    "8. データセット完成\n",
    "9. 体の向き予測モデルAを学習（損失関数：cross entropy, Arcfaceの2種類を試している）　orient_lerning.py(project10, cross entropy)　orient_lerning_metric.py(project10, ArcFace)\n",
    "10. 実験で使用する全サンプルにモデルAを適用して，各サンプルの体の向きラベルと体の向き特徴量を取得する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce86be2-4bde-47d2-9bb8-a57f05733fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "def get_image_list(folder_path):\n",
    "    # フォルダ内の画像ファイルのリストを取得\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(folder_path, ext)))\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def check_pose_detection(image_paths):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    detected_images = []\n",
    "\n",
    "    with mp_pose.Pose(static_image_mode=True, model_complexity=2) as pose:\n",
    "        for image_path in image_paths:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            # 画像をRGBに変換\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            if results.pose_world_landmarks:\n",
    "                detected_images.append(image_path)\n",
    "\n",
    "    return detected_images\n",
    "\n",
    "\n",
    "def save_detected_image_list(detected_images, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for image_path in detected_images:\n",
    "            f.write(image_path + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"C:/Users/sugie/PycharmProjects/pythonProject1/Market-1501-v15.09.15/Market-1501-v15.09.15/gt_bbox\"\n",
    "    output_file = \"detected_images_gt_bbox.txt\"\n",
    "\n",
    "    image_paths = get_image_list(folder_path)\n",
    "    detected_images = check_pose_detection(image_paths)\n",
    "    save_detected_image_list(detected_images, output_file)\n",
    "\n",
    "    print(f\"Total images checked: {len(image_paths)}\")\n",
    "    print(f\"Total images with detected poses: {len(detected_images)}\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "## mediapipeで姿勢が検出された画像のみのフォルダを作成する\n",
    "def load_detected_image_list(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        detected_images = [line.strip() for line in f.readlines()]\n",
    "    return detected_images\n",
    "\n",
    "\n",
    "def copy_detected_images(detected_images, destination_folder):\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    for image_path in detected_images:\n",
    "        if os.path.exists(image_path):\n",
    "            shutil.copy(image_path, destination_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"C:/Users/sugie/PycharmProjects/mediapipe/detected_images_gt_bbox.txt\"\n",
    "    destination_folder = \"C:/Users/sugie/PycharmProjects/pythonProject1/Market-1501-v15.09.15/Market-1501-v15.09.15/gt_bbox_detected\"\n",
    "\n",
    "    detected_images = load_detected_image_list(input_file)\n",
    "    copy_detected_images(detected_images, destination_folder)\n",
    "\n",
    "    print(f\"Total images to copy: {len(detected_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b030690-9ffa-4081-b7a6-cd9ad240380f",
   "metadata": {},
   "source": [
    "# 5. アノテーション結果csvファイルからアノテーション済みの画像ファイルを取得　　Image_list.py\n",
    "label-studioでアノテーションを行った後，label-studioないのコマンドでアノテーション結果csvファイルを出力する．\n",
    "\n",
    "csvファイルからアノテーション済みの画像リストを作成する\n",
    "\n",
    "# 6. 5のパスリストを基にアノテーション済みの画像フォルダを作成する　　Image_list.py\n",
    "以降の学習サンプルを作成するために5で取得した画像リストから，アノテーション済み画像フォルダを作成する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359e555-cf29-48c5-98f6-673dcda7b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "label-studioから出力されたアノテーションcsvファイルより，元の画像データディレクトリのうちアノテーション済みの\n",
    "画像パスを取得し，それを基にアノテーション済み画像フォルダを作成するコード\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "# CSVファイルのパスを指定\n",
    "csv_file_path = 'C:/Users//sugie/PycharmProjects/mediapipe/project-2-at-2024-07-16-03-40-eeb46e25.csv'\n",
    "\n",
    "# 抽出されたimageの部分文字列を格納するリスト（アノテーション済み画像リスト）\n",
    "image_substrings = []\n",
    "\n",
    "# CSVファイルを読み込んで処理\n",
    "with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # 空行を無視する\n",
    "        if row['image'] == '':\n",
    "            continue\n",
    "        # /data/local-files/?d=gt_bbox_detected%5以降の文字列を抽出\n",
    "        image_path = row['image']\n",
    "        substring = image_path.split('%5C', 1)[1]  #(separator, maxsplit)[リスト番号]\n",
    "        image_substrings.append(substring)\n",
    "\n",
    "# 抽出された部分文字列のリストを表示\n",
    "print(image_substrings)\n",
    "\n",
    "##以下のコードでimage_substringsに含まれる画像（アノテーション済み画像）で構成されるフォルダを作成する\n",
    "\n",
    "# 元の画像が保存されているフォルダのパス\n",
    "source_folder = 'C:/Users/sugie/PycharmProjects/pythonProject1/Market-1501-v15.09.15/Market-1501-v15.09.15/gt_bbox_detected'\n",
    "\n",
    "# 画像をコピーする新しいフォルダのパス\n",
    "destination_folder = 'C:/Users/sugie/PycharmProjects/pythonProject1/Market-1501-v15.09.15/Market-1501-v15.09.15/gt_bbox_detected_anotation'\n",
    "\n",
    "# source_folder内のファイルをリスト\n",
    "files = os.listdir(source_folder)\n",
    "\n",
    "# image_substringsリストに含まれている画像ファイルをコピーする\n",
    "for file in files:\n",
    "    # ファイル名がimage_substringsリストに含まれているかをチェック\n",
    "    for substring in image_substrings:\n",
    "        if substring in file:\n",
    "            # ファイルのフルパスを作成\n",
    "            source_file = os.path.join(source_folder, file)\n",
    "            # ファイルを新しいフォルダにコピー\n",
    "            shutil.copy(source_file, destination_folder)\n",
    "            print(f\"Copied: {source_file} to {destination_folder}\")\n",
    "            break  # マッチする部分文字列が見つかったら次のファイルへ\n",
    "\n",
    "# 追加のデバッグメッセージ\n",
    "print(\"Copy process completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f25efb2-ddc4-43f9-96f6-4068ab6708b0",
   "metadata": {},
   "source": [
    "# 7. 6で作成した画像フォルダにmedia_pipeを適用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d8fa5a-1f30-43c6-b0bd-7d87428d9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "アノテーション済みフォルダの画像ファイルにmediapipeを適用して\n",
    "file_name/choice(正解ラベル)/姿勢情報\n",
    "をcsvファイルで出力するコード\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##以下，全てのサンプルにmediapipeを適用するコード\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import cv2\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "\n",
    "# Mediapipe Poseの初期化\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# 入力フォルダのパス\n",
    "input_folder = 'C:/Users/sugie/PycharmProjects/pythonProject1/Market-1501-v15.09.15/Market-1501-v15.09.15/gt_bbox_detected_anotation'\n",
    "# 出力CSVファイルのパス\n",
    "output_csv_file = 'C:/Users/sugie/PycharmProjects/mediapipe/detected_images_gt_bbox_mediapipe.csv'\n",
    "\n",
    "# 画像のリサイズ設定\n",
    "DESIRED_HEIGHT = 128\n",
    "DESIRED_WIDTH = 64\n",
    "\n",
    "# CSVファイルのパスを指定\n",
    "csv_file_path = 'C:/Users/sugie/PycharmProjects/mediapipe/project-2-at-2024-07-16-03-40-eeb46e25.csv'\n",
    "\n",
    "# 抽出されたimageの部分文字列と対応するchoiceの値を格納するリスト\n",
    "image_substrings = []\n",
    "\n",
    "# CSVファイルを読み込んで処理\n",
    "with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # 空行を無視する\n",
    "        if row['image'] == '':\n",
    "            continue\n",
    "        # /data/local-files/?d=gt_bbox_detected%5以降の文字列を抽出\n",
    "        image_path = row['image']\n",
    "        substring = image_path.split('%5C', 1)[1]  #(separator, maxsplit)[リスト番号]\n",
    "        # 部分文字列と対応するchoiceの値をタプルとしてリストに追加\n",
    "        image_substrings.append((substring, row['choice']))\n",
    "\n",
    "# CSVのヘッダを準備\n",
    "def fields_name():\n",
    "    fields = ['file_name', 'choice']\n",
    "    for i in range(33):\n",
    "        fields.append(f'{i}_x')\n",
    "        fields.append(f'{i}_y')\n",
    "        fields.append(f'{i}_z')\n",
    "        fields.append(f'{i}_v')\n",
    "    return fields\n",
    "\n",
    "# 画像のリストを取得\n",
    "image_paths = glob.glob(os.path.join(input_folder, '*.jpg'))  # 拡張子がjpgの場合\n",
    "\n",
    "# 結果をCSVファイルに保存\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields_name())\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Mediapipe Poseの適用\n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
    "        for image_path in image_paths:\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Failed to load image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # 画像をリサイズ\n",
    "            h, w = image.shape[:2]\n",
    "            if h < w:\n",
    "                image = cv2.resize(image, (DESIRED_WIDTH, math.floor(h / (w / DESIRED_WIDTH))))\n",
    "            else:\n",
    "                image = cv2.resize(image, (math.floor(w / (h / DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
    "\n",
    "            # Mediapipe Poseを適用\n",
    "            results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            if not results.pose_world_landmarks:\n",
    "                print(f\"No landmarks detected for image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # ランドマークが6つ以上検出できないサンプルを消去\n",
    "            visible_landmarks = [landmark for landmark in results.pose_world_landmarks.landmark if landmark.visibility > 0.5]\n",
    "            if len(visible_landmarks) < 6:\n",
    "                print(f\"Not enough landmarks detected for image: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # 結果をCSVに書き込み\n",
    "            record = {'file_name': os.path.basename(image_path)}\n",
    "            # 対応するchoiceの値を追加\n",
    "            substring_key = os.path.basename(image_path)\n",
    "            choice_value = next((choice for substring, choice in image_substrings if substring in substring_key), 'N/A')\n",
    "            record['choice'] = choice_value\n",
    "            for i, landmark in enumerate(results.pose_world_landmarks.landmark):\n",
    "                record[f'{i}_x'] = landmark.x\n",
    "                record[f'{i}_y'] = landmark.y\n",
    "                record[f'{i}_z'] = landmark.z\n",
    "                record[f'{i}_v'] = landmark.visibility\n",
    "            writer.writerow(record)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc221f-cf1f-4e36-bd56-83b79990ac25",
   "metadata": {},
   "source": [
    "# 9. 体の向きを予測するモデルを学習（モデルA）\n",
    "損失関数：Cross entropy\n",
    "\n",
    "入力：1枚の人物画像にmediapipe_poseを適用して得た33のランドマークの画像内xyz座標と視認性(visavilitty)の132次元ベクトル\n",
    "\n",
    "出力：体の向き予測ラベル/前・横・後ろ（one-hot形式）\n",
    "\n",
    "- Netクラス　（3層の全結合層とドロップアウト層のニューラルネットワーク）\n",
    "- ラベルの統合とエンコーディング　（アノテーション時はカメラに対して8方向の体の向きラベルを与えていた．今回の学習では前・横・後ろの3種類を予測するため，ラベルを統合ｓいている）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082b8d7-89dc-4a6a-9aa0-bb60e8636022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 早期終了のためのクラス定義\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=0):\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "        if self.pre_loss < current_loss:\n",
    "            self.epoch += 1\n",
    "            if self.epoch > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self.epoch = 0\n",
    "            self.pre_loss = current_loss\n",
    "        return False\n",
    "\n",
    "# データセットの定義\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# ニューラルネットワークの定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.dropout_1 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(25, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.fc4(x)  # 出力にはsoftmaxを使用しない\n",
    "        return x\n",
    "\n",
    "# データの読み込み\n",
    "data_path = 'C:/Users/sugie/PycharmProjects/mediapipe/detected_images_gt_bbox_mediapipe.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 特徴量とラベルの分割\n",
    "X = data.iloc[:, 2:].values  # 3列目以降が特徴量\n",
    "y = data['choice'].values  # 2列目がラベル\n",
    "\n",
    "# NaN値を除去\n",
    "non_nan_indices = [i for i, label in enumerate(y) if label == label]  # NaNでないインデックスを取得\n",
    "X = X[non_nan_indices]\n",
    "y = y[non_nan_indices]\n",
    "\n",
    "# ラベルの統合\n",
    "label_mapping = {\n",
    "    'front': 'front',\n",
    "    'front_left': 'front',\n",
    "    'front_right': 'front',\n",
    "    'right': 'side',\n",
    "    'back_left': 'back',\n",
    "    'back': 'back',\n",
    "    'back right': 'back',\n",
    "    'left': 'side'\n",
    "}\n",
    "\n",
    "y = [label_mapping[label] for label in y]\n",
    "\n",
    "# ラベルのエンコーディング\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# データフレームに変換\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "# ラベルエンコーダーの保存\n",
    "joblib.dump(le, 'label_encoder.joblib')\n",
    "\n",
    "# 各ラベルの最小数に合わせてサンプリングする関数\n",
    "def resample_to_min_label(df):\n",
    "    # 各ラベルのサンプル数を確認\n",
    "    min_label_count = df['label'].value_counts().min()\n",
    "\n",
    "    # 各ラベルを最小数に合わせてサンプリング\n",
    "    resampled_df = df.groupby('label').apply(lambda x: x.sample(min_label_count)).reset_index(drop=True)\n",
    "\n",
    "    return resampled_df\n",
    "\n",
    "# 最小ラベル数に合わせてデータをサンプリング\n",
    "resampled_df = resample_to_min_label(df)\n",
    "\n",
    "# 特徴量とラベルに分割\n",
    "X = resampled_df.iloc[:, :-1].values\n",
    "y = resampled_df['label'].values\n",
    "\n",
    "# クラス数の確認\n",
    "num_classes = len(le.classes_)\n",
    "print(num_classes)\n",
    "\n",
    "# データの標準化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# スケーラーの保存\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# データを訓練データと一時テストデータに分割\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 一時テストデータを検証データとテストデータに分割\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 各データセットのラベルごとのサンプル数を表示\n",
    "def print_label_counts(y, dataset_name):\n",
    "    label_counts = np.bincount(y)\n",
    "    print(f\"{dataset_name} label counts:\")\n",
    "    for i, count in enumerate(label_counts):\n",
    "        print(f\"{le.classes_[i]}: {count}\")\n",
    "\n",
    "print_label_counts(y_train, \"Training data\")\n",
    "print_label_counts(y_val, \"Validation data\")\n",
    "print_label_counts(y_test, \"Test data\")\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataset = PoseDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = PoseDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "test_dataset = PoseDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# モデル、損失関数、オプティマイザーの定義\n",
    "model = Net(num_classes=num_classes, dropout_rate=0.5)  # ドロップアウト率を0.5に設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2正則化を追加\n",
    "\n",
    "# 早期終了の設定\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "# 訓練ループ\n",
    "num_epochs = 50\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "    # Early Stoppingのチェック\n",
    "    if early_stopping(epoch_val_loss):\n",
    "        break\n",
    "\n",
    "# 損失合計の表を出力\n",
    "loss_df = pd.DataFrame({'Train Loss': train_loss_history, 'Validation Loss': val_loss_history})\n",
    "print(loss_df)\n",
    "\n",
    "# 損失合計の表をCSVファイルとして保存\n",
    "loss_df.to_csv('loss_history.csv', index=False)\n",
    "\n",
    "# 損失履歴のプロット\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# モデルの保存\n",
    "model_path = 'pose_classification_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# テストループ\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "correct_by_class = [0] * num_classes\n",
    "total_by_class = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            total_by_class[label] += 1\n",
    "            if label == prediction:\n",
    "                correct_by_class[label] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 各クラスごとの正答率を計算\n",
    "for i in range(num_classes):\n",
    "    accuracy = 100 * correct_by_class[i] / total_by_class[i] if total_by_class[i] > 0 else 0\n",
    "    print(f'Accuracy for class {le.classes_[i]}: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f476a-3e1b-4885-a0fd-2533ff684df3",
   "metadata": {},
   "source": [
    "# 9. 体の向きを予測するモデルを学習（モデルA）\n",
    "損失関数：ArcFace Loss\n",
    "\n",
    "入力：1枚の人物画像にmediapipe_poseを適用して得た33のランドマークの画像内xyz座標と視認性(visavilitty)の132次元ベクトル\n",
    "\n",
    "出力：3次元ベクトル　（その後Softmax関数を適用して体の向きラベルを出力している）\n",
    "\n",
    "- Netクラス　（3層の全結合層とドロップアウト層のニューラルネットワーク）\n",
    "- ラベルの統合とエンコーディング　（アノテーション時はカメラに対して8方向の体の向きラベルを与えていた．今回の学習では前・横・後ろの3種類を予測するため，ラベルを統合しいている）\n",
    "- また距離学習の結果を可視化するために，UMAPにて可視化している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22177f0a-0995-4ff8-beb8-aca9e902f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pytorch_metric_learning.losses import ArcFaceLoss\n",
    "\n",
    "from pytorch_metric_learning import losses, distances, regularizers\n",
    "import umap\n",
    "\n",
    "# 早期終了のためのクラス定義（変更なし）\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, verbose=0):\n",
    "        self.epoch = 0\n",
    "        self.pre_loss = float('inf')\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, current_loss):\n",
    "        if self.pre_loss < current_loss:\n",
    "            self.epoch += 1\n",
    "            if self.epoch > self.patience:\n",
    "                if self.verbose:\n",
    "                    print('early stopping')\n",
    "                return True\n",
    "        else:\n",
    "            self.epoch = 0\n",
    "            self.pre_loss = current_loss\n",
    "        return False\n",
    "\n",
    "# データセットの定義（変更なし）\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# ニューラルネットワークの定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_size=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.fc4 = nn.Linear(25, embedding_size)  # 出力を3次元の埋め込みサイズに設定\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # 出力にはsoftmaxを使用しない\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# データの読み込み\n",
    "data_path = 'C:/Users/sugie/PycharmProjects/mediapipe/detected_images_gt_bbox_mediapipe.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# 特徴量とラベルの分割\n",
    "X = data.iloc[:, 2:].values  # 3列目以降が特徴量\n",
    "y = data['choice'].values  # 2列目がラベル\n",
    "\n",
    "# NaN値を除去\n",
    "non_nan_indices = [i for i, label in enumerate(y) if label == label]  # NaNでないインデックスを取得\n",
    "X = X[non_nan_indices]\n",
    "y = y[non_nan_indices]\n",
    "\n",
    "# ラベルの統合\n",
    "label_mapping = {\n",
    "    'front': 'front',\n",
    "    'front_left': 'front',\n",
    "    'front_right': 'front',\n",
    "    'right': 'side',\n",
    "    'back_left': 'back',\n",
    "    'back': 'back',\n",
    "    'back right': 'back',\n",
    "    'left': 'side'\n",
    "}\n",
    "\n",
    "y = [label_mapping[label] for label in y]\n",
    "\n",
    "# ラベルのエンコーディング\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# データフレームに変換\n",
    "df = pd.DataFrame(X)\n",
    "df['label'] = y\n",
    "\n",
    "# ラベルエンコーダーの保存\n",
    "joblib.dump(le, 'label_encoder.joblib')\n",
    "\n",
    "# 各ラベルの最小数に合わせてサンプリングする関数\n",
    "def resample_to_min_label(df):\n",
    "    # 各ラベルのサンプル数を確認\n",
    "    min_label_count = df['label'].value_counts().min()\n",
    "\n",
    "    # 各ラベルを最小数に合わせてサンプリング\n",
    "    resampled_df = df.groupby('label').apply(lambda x: x.sample(min_label_count)).reset_index(drop=True)\n",
    "\n",
    "    return resampled_df\n",
    "\n",
    "# 最小ラベル数に合わせてデータをサンプリング\n",
    "resampled_df = resample_to_min_label(df)\n",
    "\n",
    "# 特徴量とラベルに分割\n",
    "X = resampled_df.iloc[:, :-1].values\n",
    "y = resampled_df['label'].values\n",
    "\n",
    "# クラス数の確認\n",
    "num_classes = len(le.classes_)\n",
    "print(num_classes)\n",
    "\n",
    "# データの標準化\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# スケーラーの保存\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# データを訓練データと一時テストデータに分割\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 一時テストデータを検証データとテストデータに分割\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 各データセットのラベルごとのサンプル数を表示\n",
    "def print_label_counts(y, dataset_name):\n",
    "    label_counts = np.bincount(y)\n",
    "    print(f\"{dataset_name} label counts:\")\n",
    "    for i, count in enumerate(label_counts):\n",
    "        print(f\"{le.classes_[i]}: {count}\")\n",
    "\n",
    "print_label_counts(y_train, \"Training data\")\n",
    "print_label_counts(y_val, \"Validation data\")\n",
    "print_label_counts(y_test, \"Test data\")\n",
    "\n",
    "# データローダーの作成\n",
    "train_dataset = PoseDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = PoseDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "test_dataset = PoseDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# モデルの定義\n",
    "model = Net(embedding_size=3)  # 埋め込みサイズを3に設定\n",
    "\n",
    "# ArcFaceの定義\n",
    "distance = distances.CosineSimilarity()\n",
    "regularizer = regularizers.RegularFaceRegularizer()\n",
    "arcface_loss = ArcFaceLoss(num_classes=num_classes, embedding_size=3, margin=28.6, scale=64,\n",
    "                           weight_regularizer=regularizer, distance=distance)\n",
    "\n",
    "# オプティマイザーの定義（変更なし）\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "\n",
    "# 早期終了の設定（変更なし）\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "\n",
    "# 訓練ループの修正\n",
    "num_epochs = 300\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(inputs)\n",
    "        loss = arcface_loss(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    train_loss_history.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            embeddings = model(inputs)\n",
    "            loss = arcface_loss(embeddings, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    val_loss_history.append(epoch_val_loss)\n",
    "\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "    # Early Stoppingのチェック\n",
    "    if early_stopping(epoch_val_loss):\n",
    "        break\n",
    "\n",
    "# 損失合計の表を出力\n",
    "loss_df = pd.DataFrame({'Train Loss': train_loss_history, 'Validation Loss': val_loss_history})\n",
    "print(loss_df)\n",
    "\n",
    "# 損失合計の表をCSVファイルとして保存\n",
    "loss_df.to_csv('loss_history.csv', index=False)\n",
    "\n",
    "# 損失履歴のプロット\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss History')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# モデルの保存\n",
    "model_path = 'pose_classification_model_metric.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n",
    "\n",
    "# テストデータの可視化のためのUMAPの適用とプロット\n",
    "def plot_umap(embeddings, labels, n_components=2):\n",
    "    reducer = umap.UMAP(n_components=n_components, random_state=42)\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=labels, cmap='Spectral', s=5)\n",
    "    plt.colorbar(scatter, boundaries=np.arange(num_classes+1)-0.5).set_ticks(np.arange(num_classes))\n",
    "    plt.title('UMAP projection of the embeddings')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# テストループ\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "correct_by_class = [0] * num_classes\n",
    "total_by_class = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        embeddings = model(inputs)\n",
    "        outputs = arcface_loss.get_logits(embeddings)  # ArcFaceの出力からlogitsを取得\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "        for label, prediction in zip(labels, predicted):\n",
    "            total_by_class[label] += 1\n",
    "            if label == prediction:\n",
    "                correct_by_class[label] += 1\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 各クラスごとの正答率を計算（変更なし）\n",
    "for i in range(num_classes):\n",
    "    accuracy = 100 * correct_by_class[i] / total_by_class[i] if total_by_class[i] > 0 else 0\n",
    "    print(f'Accuracy for class {le.classes_[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# テストループ\n",
    "model.eval()\n",
    "test_embeddings = []\n",
    "test_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        embeddings = model(inputs)\n",
    "        test_embeddings.append(embeddings.cpu().numpy())\n",
    "        test_labels.append(labels.cpu().numpy())\n",
    "\n",
    "test_embeddings = np.concatenate(test_embeddings)\n",
    "test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# UMAPを使用して可視化\n",
    "plot_umap(test_embeddings, test_labels, n_components=2)  # 2次元での可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365b676-c379-40d0-bacf-d6825f4bdf60",
   "metadata": {},
   "source": [
    "# 10.モデルAを適用して，各サンプルの体の向きラベルと体の向き特徴量を取得する\n",
    "このコードはArcFacelossで学習したモデルAを使用している．\n",
    "\n",
    "以降の処理で自前の実験用のデータセットを使用するので，全サンプルにモデルAを適用する．\n",
    "\n",
    "#### result_dfについて\n",
    "- \"label\" 人物ラベル（体の向きラベルでないことに注意！！）\n",
    "- \"predicted_1\"-\"predicted_3\"　体の向き特徴量\n",
    "- 'predicted_label' 体の向きラベル\n",
    "\n",
    "\n",
    "（注意）\n",
    "\n",
    "現行の実験では以下のコードを使用していない．外観特徴量にCeoss entropyで出力した予測ラベルとArcFaceLossで出力した体の向き特徴量を統合している．（本当はどっちかに統一する必要あり）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8068486-1346-4154-afd4-fb5ccadd846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# データセットの定義\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# ニューラルネットワークの定義\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_size=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(132, 100)\n",
    "        self.fc2 = nn.Linear(100, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.fc3 = nn.Linear(50, 25)\n",
    "        self.fc4 = nn.Linear(25, embedding_size)  # 出力を3次元の埋め込みサイズに設定\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # 出力にはsoftmaxを使用しない\n",
    "        return x\n",
    "\n",
    "# ラベルエンコーダーと標準化スケーラーのロード\n",
    "le = joblib.load('label_encoder.joblib')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "\n",
    "# クラス数の確認\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# モデルのロード\n",
    "model_path = 'pose_classification_model_metric.pth'\n",
    "# モデルの定義\n",
    "model = Net(embedding_size=3)  # 埋め込みサイズを3に設定\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# 新しいデータの読み込み\n",
    "new_data_path = 'C:/Users/sugie/PycharmProjects/mediapipe/orientation/exp1_3Dlandmark_csv/exp13Dlandmark_all.csv'\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "# データの内容を確認\n",
    "print(new_data.head())\n",
    "print(new_data.columns)\n",
    "\n",
    "# ファイル名の抽出\n",
    "file_names = new_data['file_name'].values  # ファイル名\n",
    "\n",
    "# labelの抽出\n",
    "label_names = new_data['label'].values  # ラベル名\n",
    "\n",
    "#　point_IDの抽出\n",
    "point_ID = new_data[\"point_ID\"].values  #point_ID\n",
    "\n",
    "# 特徴量の抽出（数値データのみ）\n",
    "new_X = new_data.iloc[:, 3:].values\n",
    "\n",
    "# データの標準化\n",
    "new_X = scaler.transform(new_X)\n",
    "\n",
    "# データローダーの作成\n",
    "new_dataset = PoseDataset(torch.tensor(new_X, dtype=torch.float32), torch.tensor([0]*len(new_X), dtype=torch.long))\n",
    "new_loader = DataLoader(new_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ラベルをエンコードされた数値に変換し、結果データフレームに追加する部分\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in new_loader:\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.numpy())\n",
    "        # 各ベクトルを最も近いクラスに分類\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_labels.extend(predicted.numpy())\n",
    "\n",
    "# 予測ラベルをデコードし、テキストラベルに変換\n",
    "predicted_labels = le.inverse_transform(predicted_labels)\n",
    "\n",
    "# 結果をデータフレームに追加\n",
    "results_df = pd.DataFrame({\n",
    "    \"label\": label_names,\n",
    "    \"point_ID\": point_ID,\n",
    "    'file_name': file_names[:len(pred_1)],\n",
    "    'predicted_1': pred_1,\n",
    "    'predicted_2': pred_2,\n",
    "    'predicted_3': pred_3,\n",
    "    'predicted_label': predicted_labels  # 予測ラベルを追加\n",
    "})\n",
    "\n",
    "# CSVファイルに出力\n",
    "results_df.to_csv('predicted_exp1_metric.csv', index=False)\n",
    "print('Predictions saved to predicted_exp1_metric.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
